{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37457326-6115-4637-bd69-e4c26daeef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "# Custom CSS styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    /* Existing styles */\n",
    "    .main {\n",
    "        background-color: #1a1a1a;\n",
    "        color: #ffffff;\n",
    "    }\n",
    "    .sidebar .sidebar-content {\n",
    "        background-color: #2d2d2d;\n",
    "    }\n",
    "    .stTextInput textarea {\n",
    "        color: #ffffff !important;\n",
    "    }\n",
    "    \n",
    "    /* Add these new styles for select box */\n",
    "    .stSelectbox div[data-baseweb=\"select\"] {\n",
    "        color: white !important;\n",
    "        background-color: #3d3d3d !important;\n",
    "    }\n",
    "    \n",
    "    .stSelectbox svg {\n",
    "        fill: white !important;\n",
    "    }\n",
    "    \n",
    "    .stSelectbox option {\n",
    "        background-color: #2d2d2d !important;\n",
    "        color: white !important;\n",
    "    }\n",
    "    \n",
    "    /* For dropdown menu items */\n",
    "    div[role=\"listbox\"] div {\n",
    "        background-color: #2d2d2d !important;\n",
    "        color: white !important;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "st.title(\"üß† DeepSeek Code Companion\")\n",
    "st.caption(\"üöÄ Your AI Pair Programmer with Debugging Superpowers\")\n",
    "\n",
    "# Sidebar configuration\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Configuration\")\n",
    "    selected_model = st.selectbox(\n",
    "        \"Choose Model\",\n",
    "        [\"deepseek-r1:1.5b\", \"deepseek-r1:3b\"],\n",
    "        index=0\n",
    "    )\n",
    "    st.divider()\n",
    "    st.markdown(\"### Model Capabilities\")\n",
    "    st.markdown(\"\"\"\n",
    "    - üêç Python Expert\n",
    "    - üêû Debugging Assistant\n",
    "    - üìù Code Documentation\n",
    "    - üí° Solution Design\n",
    "    \"\"\")\n",
    "    st.divider()\n",
    "    st.markdown(\"Built with [Ollama](https://ollama.ai/) | [LangChain](https://python.langchain.com/)\")\n",
    "\n",
    "\n",
    "# initiate the chat engine\n",
    "\n",
    "llm_engine=ChatOllama(\n",
    "    model=selected_model,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "\n",
    "    temperature=0.3\n",
    "\n",
    ")\n",
    "\n",
    "# System prompt configuration\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert AI coding assistant. Provide concise, correct solutions \"\n",
    "    \"with strategic print statements for debugging. Always respond in English.\"\n",
    ")\n",
    "\n",
    "# Session state management\n",
    "if \"message_log\" not in st.session_state:\n",
    "    st.session_state.message_log = [{\"role\": \"ai\", \"content\": \"Hi! I'm DeepSeek. How can I help you code today? üíª\"}]\n",
    "\n",
    "# Chat container\n",
    "chat_container = st.container()\n",
    "\n",
    "# Display chat messages\n",
    "with chat_container:\n",
    "    for message in st.session_state.message_log:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input and processing\n",
    "user_query = st.chat_input(\"Type your coding question here...\")\n",
    "\n",
    "def generate_ai_response(prompt_chain):\n",
    "    processing_pipeline=prompt_chain | llm_engine | StrOutputParser()\n",
    "    return processing_pipeline.invoke({})\n",
    "\n",
    "def build_prompt_chain():\n",
    "    prompt_sequence = [system_prompt]\n",
    "    for msg in st.session_state.message_log:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            prompt_sequence.append(HumanMessagePromptTemplate.from_template(msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"ai\":\n",
    "            prompt_sequence.append(AIMessagePromptTemplate.from_template(msg[\"content\"]))\n",
    "    return ChatPromptTemplate.from_messages(prompt_sequence)\n",
    "\n",
    "if user_query:\n",
    "    # Add user message to log\n",
    "    st.session_state.message_log.append({\"role\": \"user\", \"content\": user_query})\n",
    "    \n",
    "    # Generate AI response\n",
    "    with st.spinner(\"üß† Processing...\"):\n",
    "        prompt_chain = build_prompt_chain()\n",
    "        ai_response = generate_ai_response(prompt_chain)\n",
    "    \n",
    "    # Add AI response to log\n",
    "    st.session_state.message_log.append({\"role\": \"ai\", \"content\": ai_response})\n",
    "    \n",
    "    # Rerun to update chat display\n",
    "    st.rerun()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
